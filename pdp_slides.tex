\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{dolphin}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{xcolor}

\title{Parameter-free Differentiable Pruning (PDP) for GPT-2}
\author{Tilde Take-Home Interview}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Overview of PDP}
\begin{itemize}
    \item PDP: \textbf{P}arameter-free \textbf{D}ifferentiable \textbf{P}runing (Cho et al.)
    \item Addresses key limitations of existing pruning methods:
    \begin{itemize}
        \item No additional trainable parameters
        \item Fully differentiable (end-to-end training)
        \item Dynamic threshold computation
        \item Progressive sparsification
    \end{itemize}
    \item Core formula: $M(w) = \sigma\left(\frac{|w| - \theta}{\tau}\right)$
    \begin{itemize}
        \item $\theta$: dynamically computed threshold based on target sparsity
        \item $\tau$: temperature parameter controlling mask sharpness
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{PDP Soft Masking Function}
\begin{center}
\includegraphics[width=0.9\textwidth]{presentation_figures/pdp_masking.png}
\end{center}
\begin{itemize}
    \item Smaller $\tau$ values create sharper transitions around threshold
    \item Allows gradients to flow during training (unlike hard binary masks)
    \item Creates a continuous mask between 0 and 1
\end{itemize}
\end{frame}

\begin{frame}{Implementation Details}
\begin{columns}
\begin{column}{0.5\textwidth}
Key components:
\begin{itemize}
    \item \textbf{PDPLinear Layer}: Wraps standard linear layers with PDP functionality
    \item \textbf{Dynamic Threshold}: Computes $k$-th smallest value where $k = \text{total\_weights} \times \text{sparsity}$
    \item \textbf{Sparsity Schedule}: Linear ramp-up from 0\% to target sparsity over warmup period
    \item \textbf{Forward Pass}: Apply soft masks during inference, while maintaining original weights for gradient computation
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{presentation_figures/tau_sparsity_heatmap.png}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Hyperparameter Sweep Results}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{presentation_figures/sparsity_perplexity.png}
\end{center}
\end{column}
\begin{column}{0.5\textwidth}
Key findings:
\begin{itemize}
    \item Optimal $\tau$ values: 0.01-0.02 (sharper masks perform better)
    \item Target sparsity up to 80\% with minimal performance degradation
    \item Critical importance of gradual sparsity increase (models with no warmup showed 15-20\% worse perplexity)
    \item Higher learning rates (3e-5 to 1e-4) work better with PDP than standard rates
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Extension 1: Improved Soft Masking}
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Enhanced masking function:}
\begin{align*}
M(w) &= 0.7 \cdot \sigma\left(\frac{|w| - \theta}{\tau}\right) \\
&+ 0.3 \cdot \left(0.5 \cdot \left(\tanh\left(\beta \cdot \frac{|w| - \theta}{\tau}\right) + 1\right)\right)
\end{align*}

\begin{itemize}
    \item Combines sigmoid + tanh components
    \item Smoother gradients around threshold
    \item $\beta$ controls transition sharpness (default: 5.0)
    \item Better training stability at high sparsity levels
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{presentation_figures/improved_masking.png}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Improved Masking Results}
\begin{center}
\includegraphics[width=0.8\textwidth]{presentation_figures/high_sparsity_comparison.png}
\end{center}
\begin{itemize}
    \item Improved masking shows 3-5\% better perplexity at high sparsity levels ($>$85\%)
    \item Particularly effective during early training phases
    \item Greater stability with large batch sizes due to smoother gradients
\end{itemize}
\end{frame}

\begin{frame}{Extension 2: Quantization-Aware PDP}
Quantization simulation during masking:
\begin{itemize}
    \item Applies $n$-bit quantization to masked weights during training
    \item Simulates real deployment constraints where both pruning + quantization are applied
    \item Forward pass: $\tilde{w} = \text{Quantize}(w \cdot M(w), \text{bits})$
    \item Backward pass: preserves original gradients for stable training
    \item Implementation extends PDPLinear with quantization option
\end{itemize}
\begin{center}
\includegraphics[width=0.75\textwidth]{presentation_figures/quantization_impact.png}
\end{center}
\end{frame}

\begin{frame}{Quantization + PDP Combined Benefits}
\begin{center}
\includegraphics[width=0.8\textwidth]{presentation_figures/model_size_reduction.png}
\end{center}
\begin{itemize}
    \item 8-bit quantization shows minimal impact (1-3\% perplexity increase)
    \item 4-bit quantization requires reducing target sparsity by 10-15\% for comparable performance
    \item Combined approach achieves up to 40Ã— size reduction (90\% sparsity + 4-bit quantization)
\end{itemize}
\end{frame}

\begin{frame}{Conclusions}
\begin{itemize}
    \item Successfully implemented and validated PDP for GPT-2 language model
    \item Demonstrated effectiveness up to 80\% sparsity with minimal perplexity impact
    \item Developed two key extensions:
    \begin{itemize}
        \item Improved soft masking for better high-sparsity performance
        \item Quantization-aware training for combined compression
    \end{itemize}
    \item Critical parameters for effective PDP deployment:
    \begin{itemize}
        \item Temperature ($\tau$): 0.01-0.02
        \item Gradual sparsity increase: Essential for performance
        \item Learning rate adjustment: Higher than standard fine-tuning
    \end{itemize}
    \item PDP offers an elegant, parameter-free approach to model compression
\end{itemize}
\end{frame}

\end{document}